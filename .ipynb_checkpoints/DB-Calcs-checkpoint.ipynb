{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries\n",
    "Import the pandas library for managing data.\n",
    "It's conventional to use this short name, esentially to\n",
    "make typing code quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set pandas options\n",
    "To limit time and space, pandas abbrevates extensive datasets when displaying them.\n",
    "Setting this option means it can display all countries in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameters.\n",
    "By assignind different values to these variables you can operate on other spreadhseets.\n",
    "These values are set for reading in the data worksheet from the World Bank's Doing Business website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_url = ('http://www.doingbusiness.org/~/media/WBG/DoingBusiness/Documents'\n",
    "            '/Data/DB18-Historical-data-complete-data-with-DTFs.xlsx') # Python concatenates the two strings\n",
    "sheet = \"All Data\"\n",
    "header_row = 1 # Python uses 0-based indexes; in the worksheet the header is in the second row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data sheet\n",
    "The data file becomes a pandas dataframe _`df`_\n",
    "* Variable / column names are extracted from the header row\n",
    "* Convert the value 'No Practice' to NaN - not a number\n",
    "\n",
    "It may take a while to download and convert the data - but not quite long enough to get impatient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_url, sheet_name=sheet, header=header_row, na_values=['No Practice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with the data\n",
    "\n",
    "Notebooks such as this allow you to interact directly with your data.\n",
    "Executing the next cell, for example, shows you a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[150:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You will notice that some of those rows (starting at 174) depart from the three-letter code standard.\n",
    "Rather than modify the data, there is code later to specifically remove them from consideration.\n",
    "\n",
    "Also the column names aren't particularly tractable, and we can do something about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the existing variable names to new names\n",
    "_Note that the import from the excel sheet includes some random line break characters `'\\n'`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rename_variables = {'Country code': 'code', \n",
    "                    'DB Year': 'year', \n",
    "                    'Procedures - Men (number) ': 's_procs',\n",
    "                    'Time - Men (days)': 's_time', \n",
    "                    'Cost - Men (% of income per capita)': 's_cost', \n",
    "                    'Minimum capital (% of income per capita)': 's_min_cap', \n",
    "                    'Procedures (number)': 'cn_procs', 'Time (days)': 'cn_time', \n",
    "                    'Cost (% of Warehouse value)': 'cn_cost', \n",
    "                    'Procedures (number).1': 'e_procs', \n",
    "                    'Time (days).1': 'e_time', \n",
    "                    'Cost (% of income per capita)': 'e_cost', \n",
    "                    'Procedures (number).2': 'rp_procs', \n",
    "                    'Time (days).2': 'rp_time', \n",
    "                    'Cost (% of property value)': 'rp_cost', \n",
    "                    'Strength of legal rights index (0-12) (DB15-18 methodology) ': 'ct_s', \n",
    "                    'Depth of credit information index (0-8) (DB15-18 methodology) ': 'ct_d', \n",
    "                    'Extent of conflict of interest regulation index (0-10)\\n(DB15-18 methodology) ':'pm_cft', \n",
    "                    'Extent of shareholder governance index (0-10) (DB15-18 methodology) ':'pm_gv', \n",
    "                    'Payments (number per year)': 't_p', 'Time (hours per year)': \n",
    "                    't_t', 'Total tax rate (% of profit)': 't_tr', \n",
    "                    'Time (days).3': 'en_time', \n",
    "                    'Cost (% of claim)': 'en_cost', \n",
    "                    'Recovery rate (cents on the dollar)': 'ri_r', \n",
    "                    'Strength of insolvency framework index (0-16) (DB15-18 methodology)': 'ri_s'\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_vars = set(df.columns.values)\n",
    "\n",
    "# Old_names from for the variables that the code keeps and renames \n",
    "old_names = set(rename_variables.keys())\n",
    "\n",
    "# The implied set of variables to drop\n",
    "vars_to_drop = all_vars - old_names\n",
    "\n",
    "# Create a list with the new names for the variables that remain \n",
    "new_names = list(rename_variables.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a copy of the dataframe\n",
    "If you want to redo the calculations interactively, just re-execute this cell\n",
    "to re-create the base dataset from the one that was read in.\n",
    "\n",
    "The first statement creates a dataframe consisting only of the wanted columns.\n",
    "The second renames each column to the new value from the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(df[list(rename_variables.keys())])\n",
    "df2.columns = list(rename_variables.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop 2013 and earlier years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['year'] > 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.drop(labels = [i for i in df2.index if df2.loc[i, 'year'] <= 2013], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines whether two data frames are, four these purposes, the same.\n",
    "A simple equality check does not suffice, since NaN values do not compare equal.\n",
    "This function was developed as a way of evaluating my refactorings (SH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same(df1, df2):\n",
    "    \"Predicate: true when all cells in two dataframes agree.\"\n",
    "    return ((df1 == df2) | (df1.isnull() & df2.isnull())).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same(df2, df3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we drop cities whose codes aren't three characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(labels = [i for i in df2.index if len(df2.loc[i,'code']) != 3], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[df3.code.apply(len)==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same(df2, df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify a multi-index for the remaining variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.set_index(['year', 'code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.set_index(['year', 'code'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build new dataframe\n",
    "Create a separate dataframe to store the normalized or distance to the frontier (DTF) values \n",
    "   for different indicators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtf = pd.DataFrame(df2.copy(deep = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-establish names\n",
    "Create a list of the remaining (non-index) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_3 = list(dtf.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indicators in high are ones where bigger values are better; opposite for variables in low \n",
    "#\n",
    "high = ['ct_s', 'ct_d', 'pm_cft', 'pm_gv',  'ri_r', 'ri_s' ]\n",
    "low = ['s_procs', 's_time', 's_cost', 's_min_cap', 'cn_procs', 'cn_time',\n",
    "       'cn_cost', 'e_procs', 'e_time', 'e_cost', 'rp_procs', 'rp_time',  \n",
    "       'rp_cost', 't_p', 't_t', 't_tr', 'en_time', 'en_cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop calculates the distance to the frontier for the 24 variables that are available in a consistent  \n",
    "   form for the years I consider, DB2014-18, or calendar 2013-17. \n",
    "\n",
    "These normalized values are stored in the dtf dataframe. The raw values remainin in the df2 dataframe. \n",
    "\n",
    "The loop defines the distance to the frontier by taking the biggest and smallest values for \n",
    "   each variable in any year from DB years 2014-18. \n",
    "\n",
    "I wrote the code as I did assuming that I would use the max and min in each year; then I found \n",
    "   that they change over time, sometimes substantially. This is the type of issue that I understand only \n",
    "   if I work directly with the data myself. \n",
    "\n",
    "This problem is that the min (worst) value for an indicator can change dramatically based on \n",
    "   what happens in a single country with a very bad business environment. \n",
    "   So I added the two lines that calculate mn_m and mx_m by taking the min and\n",
    "   max over all DB years from 2014 to 2018. This decision influences the relative influence that \n",
    "   different indicators have in my results.  \n",
    "\n",
    "This is an important point. Suppose that it takes every other country between 10 and 100 days to \n",
    "   to issue a permit, but in one laggard it takes 10,000 days. Then all other countries will have a DTF \n",
    "   score for this indicator in the range 10/10,000 to 100/10,000. In this case, a country that takes only \n",
    "   10 days gets almost no recognition for its better performance relative to a country that takes 100. \n",
    "\n",
    "The DTF value for this permit indicator will be 0.999 for the country that takes 10 days \n",
    "   and 0.990 for the country that takes 100 days. When this indicator is averaged along with 8 or 9 others, \n",
    "   it will have an effect on the overall indicator that is visible only in the third decimal place. It will \n",
    "   be swamped by variation in other indicators. \n",
    "\n",
    "My choice is not one that I would defend as being the right way to determine the relative influence of \n",
    "   different indicators. It underweights indicators with a fat lower (that is worse) tail. I haven't explored \n",
    "   the sensitivity of the results for Chile to alternative choices because I didn't want to be accused of \n",
    "   manipulating the data to get some particular outcome. \n",
    "\n",
    "For my purpose, the choice I made had the advantage that it is arbitrary and leads to rankings for \n",
    "   countries that do not change from year to year because of year to year **changes** in the min (worst) \n",
    "   value of an indicator in some lagging country. My choice ensures that he range from best to worst, \n",
    "   and hence the relative influence of each indicator, stays fixed over all the years that I consider. \n",
    "\n",
    "One of the advantages of making this code available is that it lets others do their own sensitivity analysis\n",
    "   with respect to this or any other issue. \n",
    "\n",
    "The approach used by the Doing Business team addresses this concern in a different way. \n",
    "   For most indicators (but not all), they also take the min and max over a five year interval.\n",
    "   See the description of their approach [in this report](http://www.doingbusiness.org/~/media/WBG/DoingBusiness/Documents/Annual-Reports/English/DB18-Chapters/DB18-DTF-and-DBRankings.pdf) \n",
    "\n",
    "    \n",
    "To view the values for the min and max for each variable over all years or year by year, uncomment the \n",
    "   print statements in this loop.  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(el_3)):\n",
    "    mn = df2.groupby(['year'])[el_3[i]].min()\n",
    "    #print ('mn = ', mn)\n",
    "    mn_m = mn.min()\n",
    "    #print('mn_m ',mn_m)\n",
    "    mx = df2.groupby(['year'])[el_3[i]].max()\n",
    "    #print ('mx = ', mx)\n",
    "    mx_m = mx.max()\n",
    "    #print('mx_m ',mx_m)\n",
    "    if el_3[i] in low:\n",
    "        dtf[el_3[i]] = (mx_m - df2[el_3[i]]) / (mx_m - mn_m)\n",
    "    else:\n",
    "        dtf[el_3[i]] = (df2[el_3[i]] - mn_m)  / (mx_m - mn_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[4:5]  # Test raw data for visual comparison with Bank numbers from spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf[4:5]  # Test dtf or normalized data for comparison with Bank numbers on Afghanistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Follow the Bank's hierarchical procedure; average the sub-components of the different indicators\n",
    "# \n",
    "# d_ => prefix that means \"distance to ...\"\n",
    "# s => Starting A Business ...\n",
    "# cn => Construction Permits \n",
    "# e => Getting Electricity \n",
    "# rp => Registering Property \n",
    "# ct => Contract Enforcement \n",
    "# pm => Protection for Minority investors\n",
    "# t => Taxes\n",
    "# en => Enforcing Contracts \n",
    "# ri => Resolving Insolvencies \n",
    "d_s = pd.Series((dtf['s_procs'] + dtf['s_time'] + dtf['s_cost'] + dtf['s_min_cap']) / 4)\n",
    "d_cn = pd.Series((dtf['cn_procs'] + dtf['cn_time'] +  dtf['cn_cost']) / 3)\n",
    "d_e = pd.Series((dtf['e_procs'] + dtf['e_time'] + dtf['e_cost']) / 3)\n",
    "d_rp = pd.Series((dtf['rp_procs'] + dtf['rp_time'] + dtf['rp_cost']) / 3)\n",
    "d_ct = pd.Series((dtf['ct_s'] + dtf['ct_d']) / 2) \n",
    "d_pm = pd.Series((dtf['pm_cft'] + dtf['pm_gv']) / 2)\n",
    "d_t = pd.Series((dtf['t_p'] + dtf['t_t'] + dtf['t_tr']) / 3)\n",
    "d_en = pd.Series((dtf['en_time'] + dtf['en_cost']) / 2)\n",
    "d_ri = pd.Series((dtf['ri_r'] + dtf['ri_s']) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The overall average across indicators \n",
    "# I have 9 here because none of the indicators of trade costs are available for all 5 years \n",
    "# It should be easy to tweak the code to include some of the trade indicators but \n",
    "#     at the cost of restricting the analysis to the 4 data years 2014 to 2017 \n",
    "#\n",
    "d_DTF = pd.Series((d_s + d_cn + d_e + d_rp + d_ct + d_pm + d_t + d_en + d_ri) / 9)\n",
    "#d_DTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([df2, d_s.rename('s'), \n",
    "                 d_cn.rename('cn'), \n",
    "                 d_e.rename('e'), \n",
    "                 d_rp.rename('rp'), \n",
    "                 d_ct.rename('ct'), \n",
    "                 d_pm.rename('pm'), \n",
    "                 d_t.rename('t'), \n",
    "                 d_en.rename('en'), \n",
    "                 d_ri.rename('ri'), \n",
    "                 d_DTF.rename('DTF')], axis=1)\n",
    "#df2\n",
    "#len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.dropna(axis = 0, subset = ['DTF'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df2\n",
    "#len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define 5 series, one for each year, indexed by country code\n",
    "#\n",
    "d_2018 = pd.Series(df2.loc[2018]['DTF'])\n",
    "#len(d_2018)\n",
    "d_2017 = pd.Series(df2.loc[2017]['DTF'])\n",
    "#len(d_2017)\n",
    "d_2016 = pd.Series(df2.loc[2016]['DTF'])\n",
    "#len(d_2016)\n",
    "d_2015 = pd.Series(df2.loc[2015]['DTF'])\n",
    "#len(d_2015)\n",
    "d_2014 = pd.Series(df2.loc[2014]['DTF'])\n",
    "#len(d_2014)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create series objects that I can sort, one for each year\n",
    "# \n",
    "df_2018 = pd.DataFrame(d_2018)\n",
    "dfs_2018 = df_2018.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2018)\n",
    "dfs_2018['Rank18']= pd.Series(range(1, length + 1 ,1), index=dfs_2018.index)\n",
    "\n",
    "df_2017 = pd.DataFrame(d_2017)\n",
    "dfs_2017 = df_2017.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2017)\n",
    "dfs_2017['Rank17']= pd.Series(range(1, length + 1 ,1), index=dfs_2017.index)\n",
    "\n",
    "df_2016 = pd.DataFrame(d_2016)\n",
    "dfs_2016 = df_2016.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2016)\n",
    "dfs_2016['Rank16']= pd.Series(range(1, length + 1 ,1), index=dfs_2016.index)\n",
    "\n",
    "df_2015 = pd.DataFrame(d_2015)\n",
    "dfs_2015 = df_2015.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2015)\n",
    "dfs_2015['Rank15']= pd.Series(range(1, length + 1 ,1), index=dfs_2015.index)\n",
    "\n",
    "df_2014 = pd.DataFrame(d_2014)\n",
    "dfs_2014 = df_2014.sort_values(by =['DTF'], axis=0, ascending=False, inplace=False, kind='quicksort')\n",
    "length=len(dfs_2014)\n",
    "dfs_2014['Rank14']= pd.Series(range(1, length + 1 ,1), index=dfs_2014.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe object dfs indexed by country code that holds sorted series  \n",
    "#\n",
    "# Combine the sorted series into a single dataframe indexed by country \n",
    "#\n",
    "dfs_all = pd.concat([dfs_2014, dfs_2015, dfs_2016, dfs_2017,dfs_2018], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out the data for the row of dfs_all that has the results for Chile\n",
    "#\n",
    "dfs_all.loc['CHL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
